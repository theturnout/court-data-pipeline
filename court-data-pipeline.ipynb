{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba7d5a4-ae5b-40b3-96f8-eca0092c2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from pyshacl import validate\n",
    "import glob\n",
    "import os\n",
    "import regex as re\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from rdflib import plugin\n",
    "from rdflib.graph import Graph\n",
    "from rdflib.store import Store\n",
    "from rdflib_sqlalchemy import registerplugins\n",
    "from sqlalchemy import insert\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276f6b1-fcd3-4d2a-869e-65aac08a062d",
   "metadata": {},
   "source": [
    "### Crawl Webpages and Scrape JSON-LD Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366d0030-5a62-41cd-83e6-137a74174dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 13:19:43 [scrapy.utils.log] INFO: Scrapy 2.6.3 started (bot: scrapybot)\n",
      "2022-10-07 13:19:43 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.4, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform macOS-12.5.1-x86_64-i386-64bit\n",
      "2022-10-07 13:19:43 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'CONCURRENT_REQUESTS_PER_DOMAIN': 10, 'DOWNLOAD_DELAY': 1}\n",
      "2022-10-07 13:19:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2022-10-07 13:19:43 [scrapy.extensions.telnet] INFO: Telnet Password: 18e7202320b4920a\n",
      "2022-10-07 13:19:43 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-10-07 13:19:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-10-07 13:19:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-10-07 13:19:43 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-10-07 13:19:43 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-10-07 13:19:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-10-07 13:19:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-10-07 13:19:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/no-data/il-adams.html> (referer: None)\n",
      "2022-10-07 13:19:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/no-data/vt_chittenden.html> (referer: None)\n",
      "2022-10-07 13:19:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/no-data/ca-alhambra.html> (referer: None)\n",
      "2022-10-07 13:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/no-data/nm-bernalillo.html> (referer: None)\n",
      "2022-10-07 13:19:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/no-data/mi-flint.html> (referer: None)\n",
      "2022-10-07 13:19:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/linked/il-adams.html> (referer: None)\n",
      "2022-10-07 13:19:50 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:8000\n",
      "2022-10-07 13:19:50 [urllib3.connectionpool] DEBUG: http://localhost:8000 \"GET /linked-data.json HTTP/1.1\" 200 4698\n",
      "2022-10-07 13:19:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/linked/vt_chittenden.html> (referer: None)\n",
      "2022-10-07 13:19:51 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:8000\n",
      "2022-10-07 13:19:51 [urllib3.connectionpool] DEBUG: http://localhost:8000 \"GET /linked-data.json HTTP/1.1\" 200 4698\n",
      "2022-10-07 13:19:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/linked/ca-alhambra.html> (referer: None)\n",
      "2022-10-07 13:19:52 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:8000\n",
      "2022-10-07 13:19:52 [urllib3.connectionpool] DEBUG: http://localhost:8000 \"GET /linked-data.json HTTP/1.1\" 200 4698\n",
      "2022-10-07 13:19:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/linked/nm-bernalillo.html> (referer: None)\n",
      "2022-10-07 13:19:54 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:8000\n",
      "2022-10-07 13:19:54 [urllib3.connectionpool] DEBUG: http://localhost:8000 \"GET /linked-data.json HTTP/1.1\" 200 4698\n",
      "2022-10-07 13:19:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/linked/mi-flint.html> (referer: None)\n",
      "2022-10-07 13:19:55 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:8000\n",
      "2022-10-07 13:19:55 [urllib3.connectionpool] DEBUG: http://localhost:8000 \"GET /linked-data.json HTTP/1.1\" 200 4698\n",
      "2022-10-07 13:19:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/embedded/il-adams.html> (referer: None)\n",
      "2022-10-07 13:19:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/embedded/vt_chittenden.html> (referer: None)\n",
      "2022-10-07 13:19:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/embedded/ca-alhambra.html> (referer: None)\n",
      "2022-10-07 13:20:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/embedded/nm-bernalillo.html> (referer: None)\n",
      "2022-10-07 13:20:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://localhost:8000/sites/embedded/mi-flint.html> (referer: None)\n",
      "2022-10-07 13:20:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-10-07 13:20:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 3579,\n",
      " 'downloader/request_count': 15,\n",
      " 'downloader/request_method_count/GET': 15,\n",
      " 'downloader/response_bytes': 6713948,\n",
      " 'downloader/response_count': 15,\n",
      " 'downloader/response_status_count/200': 15,\n",
      " 'elapsed_time_seconds': 18.577165,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 10, 7, 20, 20, 1, 825631),\n",
      " 'log_count/DEBUG': 26,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 104017920,\n",
      " 'memusage/startup': 104017920,\n",
      " 'response_received_count': 15,\n",
      " 'scheduler/dequeued': 15,\n",
      " 'scheduler/dequeued/memory': 15,\n",
      " 'scheduler/enqueued': 15,\n",
      " 'scheduler/enqueued/memory': 15,\n",
      " 'start_time': datetime.datetime(2022, 10, 7, 20, 19, 43, 248466)}\n",
      "2022-10-07 13:20:01 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "### dev ###\n",
    "\n",
    "# clear destination dir\n",
    "files = glob.glob(\"./raw_json/*\")\n",
    "for file in files:\n",
    "    os.remove(file)\n",
    "\n",
    "# use local files\n",
    "dev_sites = glob.glob(\"sites/*/*.html\")\n",
    "\n",
    "### /dev ###\n",
    "\n",
    "# list of sites to scrape\n",
    "sites = dev_sites\n",
    "class JsonSpider(scrapy.Spider):\n",
    "    \n",
    "    \"\"\" \n",
    "    scrape .json-ld data from court websites.\n",
    "    prefer linked .json, \n",
    "    scrape embedded data otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"court-data-spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "          \n",
    "        global sites\n",
    "        \n",
    "        # GET request, pass res to parse()\n",
    "        for url in sites:\n",
    "            yield scrapy.Request(url=f\"http://localhost:8000/{url}\", callback=self.parse)\n",
    "\n",
    "            \n",
    "    def parse(self, response):\n",
    "\n",
    "        # look for json data. \n",
    "        linked_json = response.selector.xpath(\n",
    "            '//link[@type=\"application/ld+json\"]/@href').get()\n",
    "        embedded_json = response.selector.xpath(\n",
    "            '//script[@type=\"application/ld+json\"]/text()').get()\n",
    "\n",
    "        # use page source as filename, replace \"/\" \n",
    "        # need a better convention\n",
    "        page_source = response.url.replace(\"/\", \".\")\n",
    "        filename = (page_source + \".json\").replace(\".html\",\"\")\n",
    "\n",
    "        \n",
    "        if (linked_json is not None) or (embedded_json is not None):    \n",
    "            if linked_json is not None:\n",
    "                # follow link to json file and grab data\n",
    "                req = requests.get(linked_json)\n",
    "                # to append source and date metadata below\n",
    "                load_json = json.loads(req.content)        \n",
    "            elif embedded_json is not None:\n",
    "                # parse json data from html source\n",
    "                # remove whitespace that is not in a value\n",
    "                embedded_json = re.sub(r'\\s+[^\\:\\S\\\"]', \"\", embedded_json)  \n",
    "                # to append source and date metadata below\n",
    "                load_json = json.loads(embedded_json[1:-1])\n",
    "\n",
    "            # append source and date metadata\n",
    "            load_json.append(\n",
    "                {\"source\": response.url, \"accessed\": str(datetime.datetime.now())})\n",
    "            \n",
    "            # write json file\n",
    "            json_out = json.dumps(load_json)\n",
    "            with open(f\"raw_json/{filename}\", \"w\") as output:\n",
    "                output.write(json_out)\n",
    "\n",
    "process = CrawlerProcess(\n",
    "    # requests throttled due to limitations of python http.server\n",
    "    settings = {\n",
    "        \"DOWNLOAD_DELAY\": 1,\n",
    "        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 10\n",
    "    }\n",
    ")\n",
    "\n",
    "process.crawl(JsonSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf28bbd-3967-4697-8c9b-dd85851813c4",
   "metadata": {},
   "source": [
    "### Validate JSON-LD Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75894420-629a-4c8b-890d-d9839a949b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 13:20:02 [py.warnings] WARNING: /Users/home/.local/share/virtualenvs/court-data-pipeline-DL0ZJ-Cx/lib/python3.10/site-packages/pyshacl/constraints/core/string_based_constraints.py:279: FutureWarning: Possible nested set at position 64\n",
      "  re_matcher = re.compile(re_pattern, re_flags)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files successfully validated.\n"
     ]
    }
   ],
   "source": [
    "### dev ### \n",
    "\n",
    "# clear destination dir\n",
    "files = glob.glob(\"./valid_json/*\")\n",
    "for file in files:\n",
    "    os.remove(file)\n",
    "    \n",
    "### /dev ###\n",
    "\n",
    "# scraped JSON-LD files \n",
    "scraped_json_files = glob.glob(\"./raw_json/*.json\")\n",
    "\n",
    "# SHACL file to validate data against\n",
    "shacl_file = './court-data-standard-shacl.ttl'\n",
    "\n",
    "errors = []\n",
    "\n",
    "def validate_json(scraped_json_files, shacl_file):\n",
    "    \"\"\"\n",
    "    validate scraped json files\n",
    "    if valid, move to valid_json folder\n",
    "    if not, append error msg to errors\n",
    "    \"\"\"\n",
    "\n",
    "    for file in scraped_json_files:\n",
    "        try:\n",
    "            r = validate(file,\n",
    "                         shacl_graph=shacl_file,\n",
    "                         inference='none',\n",
    "                         abort_on_first=True,\n",
    "                         allow_infos=False,\n",
    "                         allow_warnings=False,\n",
    "                         meta_shacl=False,\n",
    "                         advanced=True,\n",
    "                         js=False,\n",
    "                         debug=False)\n",
    "            \n",
    "            # if error, append msg to errors list\n",
    "            if r[0] != True:\n",
    "                msg = r[2]\n",
    "                errors.append(file + \"\\n\" + msg + \"\\n\")\n",
    "            # otherwise, move file to valid_json folder\n",
    "            else:\n",
    "                renamed_file = str(file.split(\".\")[-2] + \".json\")\n",
    "                file.replace(\"./raw_json/\",\"\")\n",
    "                os.rename(f\"{file}\", f\"./valid_json/{renamed_file}\")\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            errors.append(file + \"\\nBad JSON format. Validation aborted.\")\n",
    "            pass\n",
    "    \n",
    "    print(*errors, sep=\"\\n\") if errors else print(\"All files successfully validated.\")\n",
    "\n",
    "validate_json(scraped_json_files, shacl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55323ab-4a63-49ae-a3e6-1fe7548168a2",
   "metadata": {},
   "source": [
    "### Store JSON-LD Data in RDF... Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5273a54-073c-4e23-971a-51725dd76095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of Illinois Circuit Court http://www.illinoiscourts.gov/Circuit http://schema.org/name\n",
      "http://localhost:8000/court-data-definitions.jsonld#CourtSystem http://www.illinoiscourts.gov/Circuit http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.illinoiscourts.gov/Circuit#Circuit1District1 http://www.illinoiscourts.gov/Circuit#Circuit4 http://schema.org/areaServed\n",
      "http://www.illinoiscourts.gov/Circuit#Circuit4 http://www.illinoiscourts.gov/Circuit http://schema.org/areaServed\n",
      "State of Illinois Circuit 1 http://www.illinoiscourts.gov/Circuit#Circuit4 http://schema.org/name\n",
      "http://schema.org/AdministrativeArea http://www.illinoiscourts.gov/Circuit#Circuit4 http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "State of Illinois Circuit 1 District 1 http://www.illinoiscourts.gov/Circuit#Circuit1District1 http://schema.org/name\n",
      "http://schema.org/AdministrativeArea http://www.illinoiscourts.gov/Circuit#Circuit1District1 http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.illinoiscourts.gov/District1/CookCounty https://www.illinoiscourts.gov/courts-directory/75/Cook-County-Fifth-Municipal-District--Bridgeview/court/ http://schema.org/areaServed\n",
      "Cook County - Bridgeview http://www.illinoiscourts.gov/District1/CookCounty http://schema.org/name\n"
     ]
    }
   ],
   "source": [
    "registerplugins()\n",
    "\n",
    "valid_json_files = glob.glob(\"./valid_json/*.json\")\n",
    "SQLALCHEMY_URL = \"sqlite:///db/court_data.db\"\n",
    "\n",
    "store = plugin.get(\"SQLAlchemy\", Store)(identifier=\"court_data_store\")\n",
    "graph = Graph(store, identifier=\"court_data_graph\")\n",
    "graph.open(SQLALCHEMY_URL, create=True)\n",
    "\n",
    "files = []\n",
    "for file in valid_json_files:\n",
    "    if os.path.isfile(\"court-data.db\") == \"I'm a bird\":\n",
    "        graph.open(SQLALCHEMY_URL, create=True)\n",
    "        graph.parse(file)\n",
    "        graph.close()\n",
    "    else:\n",
    "        graph.open(SQLALCHEMY_URL)\n",
    "        graph.parse(file)\n",
    "        graph.close()\n",
    "\n",
    "graph.open(SQLALCHEMY_URL)        \n",
    "result = graph.query(\"select * where {?s ?p ?o} limit 10\")    \n",
    "for subject, predicate, object_ in result:\n",
    "    print(subject, predicate, object_)\n",
    "\n",
    "graph.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160bbca-cfa1-4415-97a4-d21dfdce856d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
